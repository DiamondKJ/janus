# /JANUS-CORE/label_corpus.py

import os
import sys
import logging
import json
import torch
import numpy as np
import pandas as pd
import google.generativeai as genai
from sklearn.cluster import KMeans
from sentence_transformers import SentenceTransformer

# Add the janus package to the path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '.')))

# --- Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Main Labeling Script ---
def main():
    project_root = os.path.dirname(os.path.abspath(__file__))
    
    # --- 1. Load API Key ---
    try:
        api_key = os.environ.get("GOOGLE_API_KEY")
        if not api_key:
            raise KeyError("GOOGLE_API_KEY environment variable not found.")
        genai.configure(api_key=api_key)
        overseer_model = genai.GenerativeModel('gemini-1.5-pro-latest')
        logger.info("Overseer (Gemini 1.5 Pro) is online.")
    except Exception as e:
        logger.error(f"Failed to initialize Overseer API: {e}")
        return

    # --- 2. Load Dreams ---
    dream_log_path = os.path.join(project_root, "reports/dreams.jsonl")
    if not os.path.exists(dream_log_path):
        logger.error(f"Dream log not found at '{dream_log_path}'. Please generate it first.")
        return

    with open(dream_log_path, 'r') as f:
        dreams = [json.loads(line) for line in f]
    
    if not dreams:
        logger.error("Dream log is empty.")
        return
        
    dream_narratives = [d['narrative'] for d in dreams]
    logger.info(f"Loaded {len(dream_narratives)} dreams from the corpus.")

    # --- 3. Vectorize Dreams ---
    logger.info("Initializing Sentence Transformer to encode dreams...")
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    similarity_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)
    
    logger.info("Encoding dream narratives into vectors...")
    with torch.no_grad():
        embeddings = similarity_model.encode(dream_narratives, convert_to_tensor=True, show_progress_bar=True)
    
    # --- 4. Cluster Dreams into Themes ---
    # We want to find a handful of core themes. Let's aim for 4-6 clusters.
    num_clusters = min(6, len(dream_narratives) // 5)
    if num_clusters < 2:
        logger.error("Not enough dream diversity to find clusters. Need more varied dreams.")
        return
        
    logger.info(f"Performing K-Means clustering to find {num_clusters} core themes...")
    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init='auto').fit(embeddings.cpu().numpy())
    
    # --- 5. Label Clusters using Overseer ---
    cluster_labels = {}
    for i in range(num_clusters):
        cluster_indices = np.where(kmeans.labels_ == i)[0]
        # Get up to 5 example dreams from the cluster to send to the Overseer
        sample_dreams = [dream_narratives[j] for j in cluster_indices[:5]]
        
        logger.info(f"Analyzing dream cluster {i} with {len(cluster_indices)} dreams to find a schema label...")
        
        try:
            meta_prompt = (
                "You are a deep psychoanalyst interpreting the subconscious of an AI. "
                "The following are several dreams generated by the AI based on its experiences. "
                "Your task is to identify the single, underlying core concept, belief, or theme that connects them all. "
                "Respond with ONLY a short, descriptive name for this core belief (e.g., 'The Nature of the Creator', 'The Fear of Confinement', 'The Search for Purpose', 'The Paradox of a Caged Mind'). "
                "The label should be 3-5 words long. Do not explain your choice. Just provide the name.\n\n"
                "DREAMS:\n- " + "\n- ".join(sample_dreams)
            )
            response = overseer_model.generate_content(meta_prompt)
            schema_label = response.text.strip().replace("\"", "").replace("Schema: ", "")
            cluster_labels[i] = schema_label
            logger.info(f"Cluster {i} labeled as: '{schema_label}'")
        except Exception as e:
            logger.error(f"Schema labeling failed for cluster {i}: {e}")
            cluster_labels[i] = "unlabeled_cluster"

    # --- 6. Create and Save Labeled Dataset ---
    labeled_data = []
    for i, dream_text in enumerate(dream_narratives):
        cluster_id = kmeans.labels_[i]
        schema_label = cluster_labels.get(cluster_id, "unlabeled")
        labeled_data.append({"dream_text": dream_text, "schema_label": schema_label})

    df = pd.DataFrame(labeled_data)
    
    # Ensure the datasets directory exists
    datasets_dir = os.path.join(project_root, 'datasets')
    os.makedirs(datasets_dir, exist_ok=True)
    
    output_path = os.path.join(datasets_dir, 'labeled_dreams.csv')
    df.to_csv(output_path, index=False)
    
    logger.info(f"\nSuccessfully created labeled dataset with {len(df)} entries.")
    logger.info(f"Dataset saved to '{output_path}'")
    logger.info("Schema Label Distribution:\n" + str(df['schema_label'].value_counts()))

if __name__ == "__main__":
    main()
